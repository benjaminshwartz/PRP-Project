{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa7d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "import boto3 as boto\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import os\n",
    "import torchvision.transforms as T \n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c98a9cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [234., 234., 234.,  ..., 234., 234., 234.],\n",
       "         [234., 234., 234.,  ..., 234., 234., 234.],\n",
       "         ...,\n",
       "         [234., 234., 234.,  ..., 234., 234., 234.],\n",
       "         [234., 234., 234.,  ..., 234., 234., 234.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [247., 247., 247.,  ..., 247., 247., 247.],\n",
       "         [247., 247., 247.,  ..., 247., 247., 247.],\n",
       "         ...,\n",
       "         [247., 247., 247.,  ..., 247., 247., 247.],\n",
       "         [247., 247., 247.,  ..., 247., 247., 247.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_image(\"patient1.png\").float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2416c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom as dicom\n",
    "import math\n",
    "import time\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "        \n",
    "    def __init__(self, data_shape, dropout = .1):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        \n",
    "        #Get data shape\n",
    "        self.in_channels, self.row_len, self.col_len = data_shape\n",
    "        \n",
    "        self.learned_embedding = nn.Parameter(torch.zeros(data_shape))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        \n",
    "        data = data + self.learned_embedding\n",
    "        data = self.dropout(data)\n",
    "        \n",
    "        return data\n",
    "\n",
    "class Convlayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,data_shape:tuple,num_patches:int,output_dim:int = None):\n",
    "        super(Convlayer,self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        self.in_channels, self.row_len, self.col_len = data_shape\n",
    "        \n",
    "        assert self.row_len % num_patches == 0 \n",
    "        assert self.col_len % num_patches == 0\n",
    "        \n",
    "        self.patch_row = self.row_len // num_patches\n",
    "        self.patch_col = self.col_len // num_patches\n",
    "        \n",
    "        self.embed_dim = int(self.in_channels * patch_row * patch_col)\n",
    "        \n",
    "        self.kernel_len = (int(patch_row), int(patch_col))\n",
    "        \n",
    "        patch_area = int(self.row_len * self.col_len)\n",
    "        \n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = self.in_channels, \n",
    "                                  out_channels = embed_dim, \n",
    "                                  kernel_size = kernel_len, \n",
    "                                  stride = kernel_len)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten(start_dim=2) \n",
    "        if output_dim == None:\n",
    "            self.dnn = nn.Linear(embed_dim,embed_dim)\n",
    "        else:\n",
    "            self.output_dim = output_dim\n",
    "            self.dnn = nn.Linear(embed_dim,output_dim)\n",
    "    def forward(self,data):\n",
    "        \n",
    "        \n",
    "        #x = self.conv2d_1(data)\n",
    "        print(x.shape)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        #x = torch.transpose(x,1,2)\n",
    "        patches = data.unfold(1,self.row_len,self.row_len).unfold(2,self.col_len,self.col_len)\n",
    "        patches = torch.reshape(patches,(self.num_patches**2,self.embed_dim))\n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.dnn(patches)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,data_shape,num_heads):\n",
    "        \n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.batch, self.patch, self.embed = data_shape\n",
    "        self.attn = nn.MultiheadAttention(self.embed,num_heads,batch_first = True)\n",
    "    def forward(self,data):\n",
    "        outputs , _ = self.attn(query=data, key=data, value=data, need_weights = False)\n",
    "        return outputs\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,data_shape,output_size,dropout = .1):\n",
    "        super(MLP,self).__init__()\n",
    "        self.batch, self.patch, self.embed = data_shape\n",
    "        hidden_output = self.embed * 2\n",
    "        self.lnn1 = nn.Linear(self.embed, hidden_output)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fnn2 = nn.Linear(hidden_output, output_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "    def forward(self,data):\n",
    "        \n",
    "        x = self.lnn1(data)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fnn2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,data_shape,num_heads,dropout=.1):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        self.batch, self.patch, self.embed = data_shape\n",
    "        self.ln1 = nn.LayerNorm([self.patch,self.embed])\n",
    "        self.ln2 = nn.LayerNorm([self.patch,self.embed])\n",
    "        self.MHA = MultiHeadAttention(data_shape,num_heads)\n",
    "        self.mlp = MLP(data_shape, output_size = self.embed, dropout=dropout)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        \n",
    "        x = self.ln1(data)\n",
    "        att_out = self.MHA(x)\n",
    "        att_out = att_out + data\n",
    "        after_ln2 = self.ln2(att_out)\n",
    "        after_ln2 = self.mlp(after_ln2)\n",
    "        after_ln2 = after_ln2 + att_out\n",
    "        \n",
    "        return after_ln2\n",
    "        \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,data_shape,num_heads,num_layers = 6,dropout = .1):\n",
    "        super(VisionTransformer,self).__init__()\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\n",
    "                f'{i}', TransformerEncoder(data_shape=data_shape,num_heads = num_heads,dropout = dropout))\n",
    "    \n",
    "    def forward(self,data):\n",
    "        x = data\n",
    "        for blk in self.blks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "    \n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self,input_layer,hidden_layer_1,hidden_layer_2,hidden_layer_3,num_output,dropout=.1):\n",
    "        super(ClassificationHead,self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(input_layer)\n",
    "        self.fnn1 = nn.Linear(input_layer,hidden_layer_1)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_layer_1)\n",
    "        self.fnn2 = nn.Linear(hidden_layer_1,hidden_layer_2)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.ln3 = nn.LayerNorm(hidden_layer_2)\n",
    "        self.fnn3 = nn.Linear(hidden_layer_2,hidden_layer_3)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        self.fnn4 = nn.Linear(hidden_layer_3,num_output)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        x = self.ln1(data)\n",
    "        x = self.fnn1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.fnn2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.ln3(x)\n",
    "        x = self.fnn3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.fnn4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PRPModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 data_shape,\n",
    "                 num_patch:int,\n",
    "                 num_heads,\n",
    "                 num_output,\n",
    "                 num_layers,\n",
    "                 conv_output_dim,\n",
    "                 hidden_layer_1,\n",
    "                 hidden_layer_2,\n",
    "                 hidden_layer_3,\n",
    "                dropout = .1):\n",
    "        super(PRPModel,self).__init__()\n",
    "        self.batch_size = data_shape[0]\n",
    "        self.data_shape = data_shape[1:]\n",
    "        \n",
    "        self.conv_layer = Convlayer(self.data_shape,num_patch,conv_output_dim)\n",
    "        self.in_channels, self.row_len, self.col_len = self.data_shape\n",
    "        assert self.row_len % num_patch == 0 \n",
    "        assert self.col_len % num_patch == 0\n",
    "        \n",
    "        patch_row = self.row_len // num_patch\n",
    "        patch_col = self.col_len // num_patch\n",
    "        \n",
    "        embed_dim = patch_row * patch_col * self.in_channels\n",
    "        assert embed_dim % num_heads == 0, f\"embed_dimension is not divisible by num_heads \\nembed_dim: {embed_dim},heads:{num_heads}\"\n",
    "        \n",
    "        self.data_shape = (self.batch_size,num_patch**2,patch_row * patch_col * self.in_channels)\n",
    "        \n",
    "        self.pos_encode = PositionalEncoding(self.data_shape,dropout)\n",
    "        \n",
    "        \n",
    "        self.visual_transformer = VisionTransformer(self.data_shape,num_heads,num_layers,dropout)\n",
    "        \n",
    "        self.input_layer = self.data_shape[1] * self.data_shape[2]\n",
    "        \n",
    "        self.ClassificationHead = ClassificationHead(self.input_layer,\n",
    "                                                     hidden_layer_1,\n",
    "                                                     hidden_layer_2,\n",
    "                                                     hidden_layer_3,\n",
    "                                                     num_output,\n",
    "                                                     dropout=.1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        x = self.conv_layer(data)\n",
    "        x = self.pos_encode(x)\n",
    "        x = self.visual_transformer(x)\n",
    "        x = torch.reshape(x,(self.batch_size,self.input_layer))\n",
    "        x = self.ClassificationHead(x)\n",
    "        x = self.softmax(x)\n",
    "        x = torch.argmax(x,axis = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e241d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not set up for multi-gpu running, need to add things to get that ready \n",
    "#Also would like to add confusion matrix output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "import boto3 as boto\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "#class Trainer:\n",
    "    \n",
    "    #def __init___(self, \n",
    "                  #model : torch.nn.Module,\n",
    "                  #optimizer: torch.optim.Optimizer,\n",
    "                  #loss_fn: torch.nn,\n",
    "                  #save_interval: int, \n",
    "                  #metric_interval: int,\n",
    "                  #train_data: DataLoader,\n",
    "                  #validation_data: DataLoader = None, \n",
    "                  #test_data: DataLoader = None,\n",
    "                  #save_path: str = None): \n",
    "        \n",
    "        #Setting all variables equal to a local counterpart \n",
    "        #self.model = model\n",
    "        #self.optimizer = optimizer\n",
    "        #self.loss_fn = loss_fn\n",
    "        #self.save_interval = save_interval\n",
    "        #self.metric_interval = metric_interval\n",
    "        #self.train_data = train_data\n",
    "        #self.validation_data = validation_data\n",
    "        #self.test_data = test_data\n",
    "        #self.save_path = save_path\n",
    "        \n",
    "        #going to be used in evaluating function to decrease latency of model \n",
    "        #self.curr_predictions = []\n",
    "        #self.curr_labels = []\n",
    "        \n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 loss_fn: torch.nn.Module,\n",
    "                 save_interval: int,\n",
    "                 metric_interval: int,\n",
    "                 train_data: DataLoader,\n",
    "                 validation_data: DataLoader = None,\n",
    "                 test_data: DataLoader = None,\n",
    "                 save_path: str = None):\n",
    "        \n",
    "        # Setting all variables equal to a local counterpart\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.save_interval = save_interval\n",
    "        self.metric_interval = metric_interval\n",
    "        self.train_data = train_data\n",
    "        self.validation_data = validation_data\n",
    "        self.test_data = test_data\n",
    "        self.save_path = save_path\n",
    "\n",
    "        # Going to be used in evaluating function to decrease latency of model\n",
    "        self.curr_predictions = []\n",
    "        self.curr_labels = []\n",
    "        \n",
    "    def _run_batch(self,batch: torch.Tensor, batch_labels: torch.Tensor):\n",
    "        #Setting current gradient to zero for new batch\n",
    "        self.optimizer.zero_grad()\n",
    "        #Running model on current batch\n",
    "        print(\"running _run_batch\")\n",
    "        pred_output = self.model(batch)\n",
    "        print(\"Done running batch\")\n",
    "        #Appending predicted values to list for evaluation \n",
    "        self.curr_predictions.append(pred_output)\n",
    "        #Appending label values to list for evaluation\n",
    "        self.curr_labels.append(batch_labels)\n",
    "        \n",
    "        #Computing loss \n",
    "        loss = self.loss_fn(pred_output,batch_labels)\n",
    "        #Computing gradient for each parameter in model\n",
    "        loss.backward()\n",
    "        #Gradient descent \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def _run_epoch(self, epoch:int):\n",
    "        #Including epoch num in init bc likely will want to print out at somepoint to see how quickly model runs\n",
    "        #Setting model to train\n",
    "        \n",
    "        self.model.train()\n",
    "        #Re-initiating prediction/label accumulator for evaluation of specific epoch\n",
    "        \n",
    "        self.curr_predictions = []\n",
    "        self.curr_labels = []\n",
    "        \n",
    "        #Looping over each training batch\n",
    "        print(\"training model\")\n",
    "        for batch_tensor, batch_labels in self.train_data:\n",
    "            \n",
    "            #Running gradient descent on each batch\n",
    "            self._run_batch(batch_tensor,batch_labels)\n",
    "        print(\"done training model\")\n",
    "        print(\"exiting _run_epoch\")\n",
    "    \n",
    "    #TODO: FINISH how to save to server\n",
    "    def _save_checkpoint(self, epoch: int):\n",
    "        #Getting the model weights at particular checkpoint\n",
    "        print(\"in save_checkpoint method\")\n",
    "        checkpoint_model = self.model.state_dict()\n",
    "        print(\"after getting the state-dict\")\n",
    "        #Pickling model into checkpoint_{epoch} file\n",
    "        #Note that pickle.dump saves model in local directory\n",
    "        #Need to delete after dump and upload\n",
    "        torch.save(checkpoint_model,f'checkpoint_{epoch}.pt')\n",
    "        #cPickle.dump(checkpoint_model, open(f'checkpoint_{epoch}.pt', 'wb'))\n",
    "        print(\"after pickle dump\")\n",
    "        #NEED TO FINISH SAVING TO FOLDER OF DICTIONARIES\n",
    "        \n",
    "    \n",
    "    def train(self, num_epochs:int):\n",
    "        \n",
    "        #Looping over number of epochs\n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            \n",
    "            #Running an epoch\n",
    "            print(f\"running {epoch} epoch\")\n",
    "            self._run_epoch(epoch)\n",
    "            \n",
    "            print(\"outside self.save_interval\")\n",
    "            #Code to save the model every save_interval   \n",
    "            if self.save_interval > 0 and epoch % self.save_interval == 0:\n",
    "                print(\"In save_interval\")\n",
    "                self._save_checkpoint(epoch)\n",
    "            #Saving the last model\n",
    "            elif epoch == num_epochs:\n",
    "                self._save_checkpoint(epoch)\n",
    "\n",
    "            #Evaluating model every metric_interval\n",
    "            print(\"outside self.metric_interval\")\n",
    "            if self.metric_interval > 0 and epoch % self.metric_interval == 0:\n",
    "                #Decreases time bc saved inferences for training data in list\n",
    "                #Evaluating Training set\n",
    "                self._evaluate(None)\n",
    "                #Will have to do inference for test set\n",
    "                if self.test_data != None:\n",
    "                    #Evaluating test set\n",
    "                    self._evaluate(self.test_data)\n",
    "                    #Resetting the model to train \n",
    "                    self._model.train()\n",
    "                \n",
    "    def _evaluate(self, dataloader: DataLoader = None):\n",
    "        #Converting to torch.no_grad to prevent gradient calculation\n",
    "        with torch.no_grad():\n",
    "            #Set model to evaluation\n",
    "            self.model.eval()\n",
    "            #If dataloader none, it means we are looking at the current training set accuracy \n",
    "            if dataloader == None:\n",
    "                #Using already predicted values that we accumulated during the training \n",
    "                #This obviously is a lower bound on the accuracy of our model on the training set\n",
    "                #However, transformer latency is extremely large so this will decrease training time overall\n",
    "                #Also we don't care about the actual training accuracy, we only care about the overall trends of \n",
    "                #training accuracy\n",
    "                predict_output = torch.vstack(self.curr_predictions)\n",
    "                labels = torch.vstack(self.curr_labels)\n",
    "                print(\"/tTRAINING SET VALUES\")\n",
    "            else:\n",
    "                #Creating accumulators for test set\n",
    "                test_predict = []\n",
    "                test_labels = []\n",
    "                #Looping over each tensor and label in the dataloader\n",
    "                for batch_tensor, batch_label in dataloader:\n",
    "                    #Predicting using model on test set\n",
    "                    prediction = self.model(batch_tensor)\n",
    "                    #accumulating model predictions and labels of test set\n",
    "                    test_predict.append(prediction)\n",
    "                    test_labels.append(batch_label)\n",
    "                #Vstacking outputs and labels so that tensors read (patient x 1)\n",
    "                #Note loss function is MSE, so output from model will be a singular value that relates to our \n",
    "                #actual scale\n",
    "                #This differs from CrossEntropyLoss, where model output would be vector of length (num_classes)\n",
    "                #and each entry would be a probability of particular class\n",
    "                predict_output = torch.vstack(test_predict)\n",
    "                labels = torch.vstack(test_labels)\n",
    "                print(\"/tTEST SET VALUES\")\n",
    "            \n",
    "            #Squeezing output to get rid of nested tensors\n",
    "            predict_output = torch.squeeze(predict_output)\n",
    "            labels = torch.squeeze(labels)\n",
    "            #Calculating loss of the model for train/test set\n",
    "            loss = self.loss_fn(predict_outputs, labels)\n",
    "            #Calculating Mean Absolute Error based on train/test set\n",
    "            MAE = (predict_output - labels).abs().mean().item()\n",
    "            \n",
    "            #Rounding predicted output so that it matches the exact categories given by Norwood scale\n",
    "            #predict_output = torch.round(predict_output)\n",
    "            \n",
    "            #Calculating how many predictions were correct\n",
    "            num_correct = (predict_output == labels).sum().item()\n",
    "            \n",
    "            #Calculating accuracy of model\n",
    "            acc = num_correct / len(labels)\n",
    "            \n",
    "            print(f\"\\t\\t NUMBER CORRECT: {num_correct}\")\n",
    "            print(f\"\\t\\t ACCURACY: {acc}\")\n",
    "            print(f\"\\t\\t MEAN ABSOLUTE ERROR: {MAE}\")\n",
    "            print(f\"\\t\\t LOSS: {loss}\")\n",
    "            print(f\"\\t\\t PREDICTED: {predict_output}\")\n",
    "            print(f\"\\t\\t LABELS: {labels}\")\n",
    "            print(f\"++++++++++++++++++++++++++++++++++++++++++++++++++++\")                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e3a85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINGLE GPU INSTANCE CAPABILITIES ONLY. WILL HAVE TO UPDATE FOR MULTIGPU\n",
    "class PRPDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, patient_ids: list,patient_labels: dict, path: str, size: tuple):\n",
    "        \n",
    "        #dictionary of where patient ids are keys and rating is value\n",
    "        self.patient_labels = patient_labels\n",
    "        \n",
    "        #list of patient ids\n",
    "        self.patient_ids = patient_ids\n",
    "        \n",
    "        #path\n",
    "        self.path = path\n",
    "        \n",
    "        #calculating mean and standard deviation along channels for entire dataset\n",
    "        mean,std = self.mean_std()\n",
    "        \n",
    "        #transformation used on each image \n",
    "        #Note that mean/std are vectors of len(4) for the alpha,RBG channels\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.Resize(size),\n",
    "            v2.ToImage(), \n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean = mean,std = std)\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def __getitem__(self,idx:int):\n",
    "        \n",
    "        #indexing via list\n",
    "        patient = self.patient_ids[idx]\n",
    "        \n",
    "        #pulling image from storage bucket and un-pickling it\n",
    "        #THIS MAY HAVE TO BE CHANGED DEPENDING ON HOW EXACTLY THE IMAGES ARE STORED\n",
    "        #image = cPickle.load(open(f'{self.path}/{patient}/image'),\"rb\")\n",
    "        image = read_image(f'{patient}.png')\n",
    "        \n",
    "        #Resize image\n",
    "        resized_image = self.transforms(image)\n",
    "        \n",
    "        #Get label\n",
    "        label = torch.tensor(self.patient_labels[patient])\n",
    "        \n",
    "        return resized_image, label\n",
    "    \n",
    "    #Should maybe try to calculate mean_std deviation once prior to running script and store data\n",
    "    #So that we do not have to rerun this step everytime the script is rerun \n",
    "    #\n",
    "    #Note we are calculating the mean of pixel average for each the image\n",
    "    #And the mean of the pixel std. dev for each the images\n",
    "    def mean_std(self):\n",
    "        #sum accumulator tensor for 4 channels x number of patients\n",
    "        sum_values = torch.zeros(4,len(self.patient_ids))\n",
    "        #std. dev accumulator tensor for 4 channels x number of patients\n",
    "        std_values = torch.zeros(4,len(self.patient_ids))\n",
    "        #Looping over photos\n",
    "        #Have to do this, cannot vectorize since the images are all of different shapes\n",
    "        for i in range(len(self.patient_ids)):\n",
    "            #Getting patient\n",
    "            patient = self.patient_ids[i]\n",
    "            #opening photo from path\n",
    "            #Make sure to pickle these images as Tensor objects\n",
    "            #image = cPickle.load(open(f'{patient}.png',\"rb\"))\n",
    "            image = read_image(f'{patient}.png').float()\n",
    "            #Getting pixel average of the photo\n",
    "            pixel_avg = torch.mean(image, dim = (1,2))\n",
    "            #Storing patient i in ith row of sum value\n",
    "            sum_values[:,i] = pixel_avg\n",
    "            #Getting pixel std. dev of the photo\n",
    "            pixel_std = torch.std(image, dim = (1,2))\n",
    "            #Storing patient i std. dev ith row of std. dev matrix\n",
    "            std_values[:,i] = pixel_std\n",
    "        \n",
    "        #Calculating average mean and std. dev. values\n",
    "        a = torch.mean(sum_values, axis = 1)\n",
    "        b = torch.mean(std_values, axis = 1)\n",
    "        \n",
    "        return a,b\n",
    "    \n",
    "def sequential_train_test_split(split: tuple, labels:dict):\n",
    "    \n",
    "    #Getting number of patients \n",
    "    num_patients = len(labels.keys())\n",
    "    #Getting how patients to include in training\n",
    "    training_num = int(split[0] * num_patients)\n",
    "    #creating training set\n",
    "    training_patients = labels.keys()[:training_num]\n",
    "    #Creating Test set \n",
    "    testing_patients = labels.keys()[training_num:]\n",
    "\n",
    "    return training_patients, testing_patients\n",
    "\n",
    "def random_train_test_split(split: tuple, labels:dict):\n",
    "    \n",
    "    #Converting label list to label set to use subtraction\n",
    "    patient_set = set(labels.keys())\n",
    "    \n",
    "    #Traning set\n",
    "    training_patients = set()\n",
    "\n",
    "    #calculating number of patients\n",
    "    num_patients = len(labels.keys())\n",
    "    \n",
    "    #calculating number of patients in training set\n",
    "    training_num = int(split[0] * num_patients)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    #looping number of patients \n",
    "    while i != training_num:\n",
    "        #Choosing a patient from the list of left over patients not yet chosen\n",
    "        curr_patient = random.choice(list(patient_set-training_patients))\n",
    "        #adding patient to list of training patient\n",
    "        training_patients.add(curr_patient)\n",
    "        #Iterating\n",
    "        i += 1\n",
    "\n",
    "    #Getting test set\n",
    "    test_patients = patient_set - training_patients\n",
    "\n",
    "    #Reconverting back to list \n",
    "    return list(training_patients), list(test_patients)\n",
    "\n",
    "def get_train_test_dataset(dict_path: str, data_path: str, size: tuple ,sequential: bool, split:tuple):\n",
    "    \n",
    "    #Getting dictionary that has patient keys and scores as values\n",
    "    #Path may need to be changed\n",
    "    patient_labels = cPickle.load(open(dict_path, 'rb'))\n",
    "    \n",
    "    #If want the data to be split sequentially (i.e in order)\n",
    "    if sequential:\n",
    "        train, test = sequential_train_test_split(split,patient_labels)\n",
    "    else:\n",
    "    #If want data split randomly (More Likely used)\n",
    "        train, test = random_train_test_split(split,patient_labels)\n",
    "    \n",
    "    #Creating PRPDataSet objects for both train and test sets\n",
    "    train_set = PRPDataSet(train,patient_labels,data_path,size)\n",
    "    test_set = PRPDataSet(train,patient_labels,data_path,size)\n",
    "    \n",
    "    return train_set,test_set\n",
    "\n",
    "def PRPDataLoader(dict_path: str, data_path: str, size: tuple,sequential: bool, split: tuple, batch: int):\n",
    "    \n",
    "    #Getting PRPDataSet objects for both rain and test sets \n",
    "    train_set, test_set = get_train_test_dataset(dict_path, data_path, size ,sequential, split)\n",
    "    \n",
    "    #Creating DataLoader with Train and Set Data sets\n",
    "    train_generator = DataLoader(train_set, batch_size = batch, shuffle = True)\n",
    "    test_generator = DataLoader(test_set, batch_size = batch, shuffle = True)\n",
    "    \n",
    "    return train_generator,test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aff14154",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list = [\"patient1\",\"patient2\"]\n",
    "test_dict = cPickle.load(open(\"test_dict\",\"rb\"))\n",
    "data = PRPDataSet(patient_list,test_dict,\"\",(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce1dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = PRPDataLoader(\"test_dict\",\"\",(200,200),False,(1,0),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c57fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PRPModel((2,4,200,200),10,10,2,2,None,100,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e2e586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    \n",
    "mse_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "753f1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model,\n",
    "                  adam_optimizer,\n",
    "                  mse_loss,\n",
    "                  100,\n",
    "                  1,\n",
    "                  train_data = data,\n",
    "                  test_data= DataLoader([x]),\n",
    "                  save_path = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df30e7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64046502"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "46c06335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 1 epoch\n",
      "training model\n",
      "running _run_batch\n",
      "torch.Size([1600, 10, 10])\n",
      "torch.Size([1600, 10, 10])\n",
      "torch.Size([1600, 10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16000x10 and 1600x1600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m#Running an epoch\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutside self.save_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m#Code to save the model every save_interval   \u001b[39;00m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mTrainer._run_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_tensor, batch_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data:\n\u001b[1;32m    108\u001b[0m     \n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m#Running gradient descent on each batch\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexiting _run_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mTrainer._run_batch\u001b[0;34m(self, batch, batch_labels)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#Running model on current batch\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning _run_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m pred_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone running batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#Appending predicted values to list for evaluation \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36mPRPModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,data):\n\u001b[0;32m--> 216\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encode(x)\n\u001b[1;32m    218\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_transformer(x)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36mConvlayer.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(x,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16000x10 and 1600x1600)"
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"patient1\" : 3, \"patient2\" : 3}\n",
    "cPickle.dump(labels_dict, open(f'test_dict', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b930dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4,200,200)\n",
    "patches = x.unfold(1, 20, 20).unfold(2, 20, 20)\n",
    "patches = torch.reshape(patches,(100,4 * 20 * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "348b6a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1600])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "47849b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 200, 20])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice.unfold(1,20,20).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
